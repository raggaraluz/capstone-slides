Text prediction application
========================================================
author: Rafael Garcia
date: `r format(Sys.time(), "%Y-%m-%d")`
autosize: true
transition: zoom
transition-speed: default
font-family: 'Arial'

<style>

/* ordered and unordered list styles */
.reveal h1, .reveal h2, .reveal h3 {
  word-wrap: normal;
  -moz-hyphens: none;
}

.small-code pre code {
  font-size: 0.9em;
}

.reveal ul, 
.reveal ol {
    font-size: 0.8em;
}

.reveal ul ul,
.reveal ul ol,
.reveal ol ol,
.reveal ol ul {
	font-size: 0.7em;
}

.reveal ul ul, 
.reveal ol ul {
    list-style-type: circle;
}

.reveal p {
    font-size: 0.9em;
}

.footer {
    position: fixed;
    top: 90%;
    text-align:left;
    width:100%;
}



.small-code pre code {
  font-size: 1em;
}

</style>


```{r settings, include=FALSE}
library(RColorBrewer)
library(knitr)
library(quanteda)
library(dplyr)
library(plotly)
library(ggplot2)

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

<div class="footer" style="margin-top:-50px;background-color:transparent;font-size:60px;color:white;top:78%">
Coursera data science specialization
</div>
<div class="footer" style="margin-top:-50px;background-color:transparent;font-size:40px;color:white";top:80%>
Capstone project
</div>
<div class="footer" style="margin-top:-50px;background-color:transparent;font-size:30px;color:white;top:100%">Live application available <a href="https://raggaraluz.shinyapps.io/text-predictor/">here</a>. Code available <a href="https://github.com/raggaraluz/capstone-shiny-app">here</a>
</div>


Application Features
========================================================
incremental: true

### Internals

* Reads text input in English which is tokenized into words and cleaned by removing
    * numbers
    * punctuation symbols
    * uncommon words
* Based on the text input, it will predict the three most likely words that will follow in the text

***

### Graphical Interface

* Multi-line input text area for free writing and copy/paste
* Prediction is offered with three labeled buttons
    * Clicking on any button will add the selected word to the text automatically
* Stats section to summarize each prediction
    * Base text used for the prediction
    * Likelihood of the prediciton based on the model

```{r calculations, include=FALSE, cache=TRUE}
sentences <- readLines(file('../data/reduced_dataset.txt', 'rb'), encoding = 'UTF-8')
# sentences <- sample(sentences, length(sentences) * .2)
entries <- length(sentences)

docs <- corpus(sentences)
rm(sentences)

tokensAll <- tokenize(toLower(docs), removeNumbers = T, removePunct = T, removeSeparators = T,
                      removeTwitter = T, removeSymbols = T, removeURL = T)

dist.1 <- dfm(tokensAll)
top.1 <- topfeatures(dist.1, 1e10)
```    


Training data
========================================================
```{r word_filtering, include=FALSE}
cdf.1 <- cumsum(top.1) / sum(top.1)
percs <- c(0.8, 0.9, 0.95)


percs.text <- paste0(100*percs, '%')
words.perc <- sapply(percs, function(a) { which(cdf.1 > a)[1] } )

```
* Data used for training the prediction model was obtained from HC Corpora (http://www.corpora.heliohost.org/)
    * Containing more than 4 million entries from blogs, newspapers and twitter
    * It was sampled to the 10% (about `r round(entries / 1000)`k entries) due to memory and processing capacity constraints
    * It contains about `r round(sum(top.1)/1e6)`M total words, with `r round(length(top.1)/1e3)`k distinct words
* Non-common words are removed from training dataset and application input

*** 

* Target coverage of `r percs.text[length(percs.text)]` keeping only `r paste0(ceiling(100*words.perc[length(words.perc)] / length(cdf.1)), '%')` of the words
```{r coverage_plot, results='asis'}
ggplot(mapping = aes(x = seq_along(cdf.1), y = cdf.1)) +
        geom_line(color = 'blue') +
        geom_hline(yintercept = percs, lty = 2, color = 'red') +
        geom_vline(xintercept = words.perc, color = 'red', lty = 2) +
        geom_label(aes(x=0, y = percs, label = percs.text)) +
        geom_label(aes(y=0.05, x = words.perc, label = words.perc)) +
        scale_y_continuous(labels=scales::percent) +
        xlab('Number of words') + ylab('Coverage') + ggtitle('Coverage vs number of words')
```

Prediction model
=======================================================
* Natural language text model is based on Markov Chains
    * Each state is represented as a N-gram
    * The follwoing state (which would include the following word) would only depend on the previous state (previous N-gram)
* This simplify the model by assuming next word only depends on the last N words and not all the text

***
    
* The prediction model was built based on the document-feature matrix for *N*-grams
    * It calculates distribution of the last word of the *N*-gram depending (grouped by) the first *N-1* words
* The prediction is the most common three words calculated in the previous step
* The model will be applied by picking the last *N-1* words of the text and getting the three predictions given by the model

Back-off model
========================================================
left: 50%
class: small-code

* Default model uses *N=4*
    * Trained using 4-grams and prediction based on the last *3* words
* In case of not enough predictions, it will back-off to the model with *N=3*
    * Trained with 3-grams and prediction based on last 2 words
* Model will back off to *N=2* and *N=1* if required

*** 
```{r eval=FALSE, echo=TRUE}
### SIMPLIFIED CODE
next.word <- function(input) {
 tk <- get.tokens(input)
 l <- length(tk)
 if (l >= 3) {
     p <- prediction(tk[l-2], tk[l-1], tk[l], words = 3)
 }
 if (length(p) < 3 && l >= 2) {
     p <- c(p, prediction(tk[l-1], tk[l], words = 3 -  length(p))) 
 }
 if (length(p) < 3 && l >= 1) {
     p <- c(p, prediction(tk[l], words = 3 - length(p)))
 }
 if (length(p) < 3) {
     p <- c(p, prediction(words = 3 - length(p)))
 }
 p
}

```
